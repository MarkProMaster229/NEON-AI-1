NEON-AI-1 — Минимальный чат-бот на PyTorch

Этот проект реализует базового чат-бота на основе LSTM-модели в PyTorch, обученного на паре текстов "Пользователь: ... / Бот: ..." из текстового файла. Цель — продемонстрировать простой пайплайн: от загрузки данных и токенизации до обучения модели и генерации ответов.


Зависимости

- Python 3.7+
- PyTorch (`torch`)
- Опционально: `CUDA`, если хочешь обучать на GPU

Установи зависимости через pip:


pip install torch
 Формат входных данных
Файл data.txt должен быть в следующем виде:

makefile
Копировать
Редактировать
Пользователь: Привет
Бот: Привет! Чем могу помочь?
Пользователь: Что ты умеешь?
Бот: Я умею отвечать на простые вопросы.
Диалоги должны чередоваться: вопрос → ответ → вопрос → ответ и так далее.

 Как запустить
Скачай или клонируй репозиторий.

Убедись, что файл data.txt лежит в корне проекта и содержит пары "Пользователь / Бот".

Укажи путь к data.txt в переменной file_path в chatbot.py.

python
Копировать
Редактировать
file_path = r"C:\твой\путь\к_этому_файлу\NEON-AI-1\data.txt"
Запусти chatbot.py:

bash
Копировать
Редактировать
python chatbot.py
Скрипт:

загрузит файл;

обучит модель (если нет сохраненной);

сохранит веса в chatbot_model.pth;

сгенерирует ответ на тестовую фразу Что ты умеешь?.

 Что делает скрипт?
Загружает и обрабатывает диалоги.

Строит словарь всех слов.

Токенизирует вопросы/ответы.

Паддит последовательности.

Создает и обучает LSTM-модель.

Сохраняет веса модели.

Генерирует ответ на входной вопрос.

 Архитектура модели
Embedding слой для векторного представления слов

LSTM для обработки последовательностей

Linear для вывода распределения по словарю

 Пример вывода

 Примечания
Веса сохраняются в chatbot_model.pth. Если он уже существует, модель не будет переобучаться.

Если слово не найдено в словаре, оно будет заменено на <UNK>.

Все слова приводятся к нижнему регистру и очищаются от пунктуации.

Размерность словаря +1 (0 — для паддинга).

 Потенциальные улучшения
Добавить нормализацию текста (удаление стоп-слов и пр.).

Расширить архитектуру (двунаправленный LSTM, attention).

Добавить поддержку нескольких фраз в ответах.

Перевести на GPU (model.to('cuda') и inputs.to('cuda') и пр.).

Сохранение и загрузка word_to_index и index_to_word (сейчас они теряются между запусками).
